#!/usr/bin/env python3
"""
Helios Trading Bot - Phase 1.3 Data Pipeline Testing Script

This script tests the complete data pipeline implementation:
1. Connection managers (Neon PostgreSQL, Upstash Redis, Cloudflare R2)
2. Database schema creation and validation
3. Market data pipeline functionality
4. Data quality monitoring
5. Performance metrics

Usage:
    python scripts/test_data_pipeline.py

Features:
- Comprehensive testing of all pipeline components
- Real-time feedback with progress indicators
- Detailed error reporting and diagnostics
- Performance benchmarking
- Data validation and quality checks
"""

import asyncio
from datetime import datetime, timezone
from decimal import Decimal
from pathlib import Path
import sys
import time
from typing import Dict

# Add project root to Python path BEFORE local imports
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

from src.api.models import TickerData
from src.core.config import get_config, load_configuration
from src.data.connection_managers import ConnectionManager
from src.data.database_schema import DatabaseSchema, initialize_database
from src.data.market_data_pipeline import MarketDataPipeline
from src.utils.logging import get_logger

logger = get_logger(__name__)


class PipelineTestSuite:
    """Comprehensive test suite for data pipeline."""

    def __init__(self):
        self.results = {
            "start_time": datetime.now(),
            "tests_run": 0,
            "tests_passed": 0,
            "tests_failed": 0,
            "errors": [],
            "performance_metrics": {},
        }

        # Track all test objects created for cleanup
        self.test_objects = {
            "redis_keys": set(),
            "database_symbols": set(),
            "database_tables_to_clean": {
                "current_prices": set(),
                "data_quality_metrics": set(),
                "ohlcv_1m": set(),
                "ohlcv_5m": set(),
                "ohlcv_1h": set(),
                "ohlcv_4h": set(),
                "ohlcv_1d": set(),
            },
        }

        # Initialize connection manager for cleanup
        self.cleanup_manager = None

    def print_header(self, title: str):
        """Print formatted test section header."""
        print(f"\n{'='*60}")
        print(f"ðŸ§ª {title}")
        print(f"{'='*60}")

    def print_status(self, message: str, status: str = "info"):
        """Print formatted status message."""
        icons = {
            "info": "â„¹ï¸",
            "success": "âœ…",
            "warning": "âš ï¸",
            "error": "âŒ",
            "running": "ðŸ”„",
        }

        icon = icons.get(status, "â„¹ï¸")
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {icon} {message}")

    def record_test_result(
        self, test_name: str, success: bool, error: str = None, metrics: Dict = None
    ):
        """Record the result of a test."""
        self.results["tests_run"] += 1

        if success:
            self.results["tests_passed"] += 1
            self.print_status(f"âœ… {test_name}: PASSED")
        else:
            self.results["tests_failed"] += 1
            self.print_status(f"âŒ {test_name}: FAILED")

            if error:
                self.results["errors"].append({"test": test_name, "error": error})
                self.print_status(f"   Error: {error}", "error")

        if metrics:
            self.results["performance_metrics"][test_name] = metrics

    def track_test_object(self, object_type: str, identifier: str, table: str = None):
        """Track test objects for cleanup."""
        if object_type == "redis_key":
            self.test_objects["redis_keys"].add(identifier)
        elif object_type == "database_symbol":
            self.test_objects["database_symbols"].add(identifier)
            if table and table in self.test_objects["database_tables_to_clean"]:
                self.test_objects["database_tables_to_clean"][table].add(identifier)

    async def initialize_cleanup_manager(self):
        """Initialize connection manager for cleanup operations."""
        if not self.cleanup_manager:
            self.cleanup_manager = ConnectionManager()
            await self.cleanup_manager.connect_all()

    async def cleanup_all_test_objects(self):
        """Comprehensive cleanup of all test objects created during testing."""
        try:
            self.print_status("ðŸ§¹ Cleaning up test objects...", "info")
            await self.initialize_cleanup_manager()
            cleanup_timeout = 30
            await asyncio.wait_for(self._do_cleanup(), timeout=cleanup_timeout)
            self.print_status("âœ… Test cleanup completed", "success")
        except asyncio.TimeoutError:
            self.print_status("âš ï¸ Cleanup timed out but continuing", "warning")
        except Exception as e:
            self.print_status(f"Cleanup error (non-critical): {e}", "warning")
        finally:
            if self.cleanup_manager:
                try:
                    await self.cleanup_manager.disconnect_all()
                except Exception:
                    pass

    async def _do_cleanup(self):
        """Execute the actual cleanup operations."""
        config = get_config()
        schema_name = config.database_schema

        await self._cleanup_redis_keys()
        await self._cleanup_database_symbols(schema_name)
        await self._cleanup_test_patterns(schema_name)

    async def _cleanup_redis_keys(self):
        """Clean up a limited number of redis keys."""
        if self.test_objects["redis_keys"]:
            redis_keys = list(self.test_objects["redis_keys"])[:20]
            self.print_status(f"Cleaning {len(redis_keys)} Redis keys", "info")
            for redis_key in redis_keys:
                try:
                    await self.cleanup_manager.redis.delete(redis_key)
                except Exception:
                    pass

    async def _cleanup_database_symbols(self, schema_name):
        """Clean up database entries for tracked symbols."""
        if self.test_objects["database_symbols"]:
            symbols = list(self.test_objects["database_symbols"])
            self.print_status(f"Cleaning {len(symbols)} database symbols", "info")
            for table in ["current_prices", "data_quality_metrics"]:
                try:
                    placeholders = ", ".join(f"${i+1}" for i in range(len(symbols)))
                    delete_sql = f"DELETE FROM {schema_name}.{table} WHERE symbol IN ({placeholders})"
                    await self.cleanup_manager.postgres.execute(delete_sql, *symbols)
                except Exception:
                    pass

    async def _cleanup_test_patterns(self, schema_name):
        """Clean up test data by recognizing common test patterns."""
        test_patterns = [
            "TEST%",
            "BENCH%",
            "QUALITY%",
            "POOR%",
            "FALLBACK%",
            "METRICS%",
            "VALID%",
        ]
        for pattern in test_patterns:
            try:
                await self.cleanup_manager.postgres.execute(
                    f"DELETE FROM {schema_name}.current_prices WHERE symbol LIKE $1",
                    pattern,
                )
                await self.cleanup_manager.postgres.execute(
                    f"DELETE FROM {schema_name}.data_quality_metrics WHERE symbol LIKE $1",
                    pattern,
                )
            except Exception as e:
                self.print_status(f"DB cleanup failed for {pattern}: {e}", "warning")

            if pattern.endswith("%"):
                base_pattern = pattern[:-1]
                common_keys = [
                    f"price:{base_pattern}USDT",
                    f"ticker:{base_pattern}USDT",
                ]
                for key in common_keys:
                    try:
                        await self.cleanup_manager.redis.delete(key)
                    except Exception:
                        pass

    async def test_configuration(self) -> bool:
        """Test 1: Configuration and Environment Variables."""
        self.print_header("Test 1: Configuration Validation")

        try:
            # Load configuration first
            config = load_configuration()

            # Check required environment variables
            required_vars = [
                ("neon_host", "NEON_HOST"),
                ("neon_database", "NEON_DATABASE"),
                ("neon_username", "NEON_USERNAME"),
                ("neon_password", "NEON_PASSWORD"),
                ("upstash_redis_host", "UPSTASH_REDIS_HOST"),
                ("upstash_redis_password", "UPSTASH_REDIS_PASSWORD"),
                ("r2_account_id", "R2_ACCOUNT_ID"),
                ("r2_api_token", "R2_API_TOKEN"),
                ("r2_bucket_name", "R2_BUCKET_NAME"),
            ]

            missing_vars = []
            for attr, env_var in required_vars:
                value = getattr(config, attr, None)
                if not value:
                    missing_vars.append(env_var)
                else:
                    # SECURITY: Never print passwords or tokens - only show presence
                    if any(
                        sensitive in env_var.lower()
                        for sensitive in ["password", "secret", "token", "key"]
                    ):
                        self.print_status(f"{env_var}: [CONFIGURED SECURELY]")
                    else:
                        # For non-sensitive values, show truncated version
                        masked_value = (
                            value[:8] + "..." if len(value) > 8 else value[:3] + "..."
                        )
                        self.print_status(f"{env_var}: {masked_value}")

            if missing_vars:
                error_msg = f"Missing environment variables: {', '.join(missing_vars)}"
                self.record_test_result("Configuration Check", False, error_msg)
                return False

            self.record_test_result("Configuration Check", True)
            return True

        except Exception as e:
            self.record_test_result("Configuration Check", False, str(e))
            return False

    async def test_connection_managers(self) -> bool:
        """Test 2: Connection Managers."""
        self.print_header("Test 2: Connection Managers")

        manager = None
        try:
            start_time = time.time()
            manager = ConnectionManager()

            # Test individual connections
            self.print_status("Testing PostgreSQL connection...", "running")
            await manager.postgres.connect()
            pg_health = await manager.postgres.health_check()

            self.print_status("Testing Redis connection...", "running")
            await manager.redis.connect()
            redis_health = await manager.redis.health_check()

            self.print_status("Testing R2 connection...", "running")
            await manager.r2.connect()
            r2_health = await manager.r2.health_check()

            connection_time = time.time() - start_time

            # Validate health
            all_healthy = all(
                [pg_health.is_healthy, redis_health.is_healthy, r2_health.is_healthy]
            )

            if all_healthy:
                self.print_status(f"PostgreSQL: {pg_health.response_time_ms:.1f}ms")
                self.print_status(f"Redis: {redis_health.response_time_ms:.1f}ms")
                self.print_status(f"R2: {r2_health.response_time_ms:.1f}ms")

                self.record_test_result(
                    "Connection Managers",
                    True,
                    metrics={
                        "total_connection_time": connection_time,
                        "postgresql_response_time": pg_health.response_time_ms,
                        "redis_response_time": redis_health.response_time_ms,
                        "r2_response_time": r2_health.response_time_ms,
                    },
                )
                return True
            else:
                unhealthy = []
                if not pg_health.is_healthy:
                    unhealthy.append(f"PostgreSQL: {pg_health.error_message}")
                if not redis_health.is_healthy:
                    unhealthy.append(f"Redis: {redis_health.error_message}")
                if not r2_health.is_healthy:
                    unhealthy.append(f"R2: {r2_health.error_message}")

                error_msg = "Unhealthy connections: " + "; ".join(unhealthy)
                self.record_test_result("Connection Managers", False, error_msg)
                return False

        except Exception as e:
            self.record_test_result("Connection Managers", False, str(e))
            return False
        finally:
            if manager:
                await manager.disconnect_all()

    async def test_database_schema(self) -> bool:
        """Test 3: Database Schema Creation and Validation."""
        self.print_header("Test 3: Database Schema")

        try:
            start_time = time.time()

            self.print_status("Creating database schema...", "running")
            success = await initialize_database()

            if not success:
                self.record_test_result(
                    "Database Schema", False, "Schema initialization failed"
                )
                return False

            # Verify schema
            schema = DatabaseSchema()
            verification = await schema.verify_schema()
            schema_time = time.time() - start_time

            if verification["tables_exist"]:
                self.print_status(f"Created {verification['tables_count']} tables")
                self.print_status(f"Created {verification['indexes_count']} indexes")

                # Test basic operations
                await self._test_basic_database_operations(schema)

                self.record_test_result(
                    "Database Schema",
                    True,
                    metrics={
                        "schema_creation_time": schema_time,
                        "tables_created": verification["tables_count"],
                        "indexes_created": verification["indexes_count"],
                    },
                )
                return True
            else:
                error_msg = f"Schema verification failed: {verification}"
                self.record_test_result("Database Schema", False, error_msg)
                return False

        except Exception as e:
            self.record_test_result("Database Schema", False, str(e))
            return False

    async def _test_basic_database_operations(self, schema: DatabaseSchema):
        """Test basic database operations."""
        try:
            # Test insert/select operations
            test_symbol = "TESTBTCUSDT"

            # Track test object for cleanup
            self.track_test_object("database_symbol", test_symbol, "current_prices")

            # Insert test data using schema-qualified table name
            await schema.connection_manager.postgres.execute(
                f"""INSERT INTO {schema.schema_name}.current_prices (symbol, price, timestamp)
                   VALUES ($1, $2, CURRENT_TIMESTAMP)
                   ON CONFLICT (symbol) DO UPDATE SET price = EXCLUDED.price""",
                test_symbol,
                Decimal("50000.0"),
            )

            # Verify data
            result = await schema.connection_manager.postgres.fetchrow(
                f"SELECT * FROM {schema.schema_name}.current_prices WHERE symbol = $1",
                test_symbol,
            )

            if result and result["symbol"] == test_symbol:
                self.print_status("Database operations: INSERT/SELECT working")
            else:
                raise Exception("Failed to insert/retrieve test data")

            # Note: Cleanup is now handled centrally, no manual cleanup needed here

        except Exception as e:
            self.record_test_result(
                "_test_basic_database_operations",
                False,
                f"Valid data insert failed: {e}",
            )
            raise Exception(f"Valid data insert failed: {e}") from e

    async def test_market_data_pipeline(self) -> bool:
        """Test 4: Market Data Pipeline."""
        self.print_header("Test 4: Market Data Pipeline")

        try:
            start_time = time.time()

            # Create a fresh connection manager for this test
            from src.data.connection_managers import ConnectionManager

            test_manager = ConnectionManager()
            await test_manager.connect_all()

            # Create test pipeline with isolated connection
            pipeline = MarketDataPipeline()
            pipeline.connection_manager = test_manager  # Use our fresh manager

            # Initialize Binance client
            from src.api.binance_client import BinanceClient

            pipeline.binance_client = BinanceClient(pipeline.config)

            # Test current price storage and retrieval
            test_ticker = TickerData(
                symbol="BTCUSDT",
                price=Decimal("45000.00"),
                bid_price=Decimal("44999.00"),
                ask_price=Decimal("45001.00"),
                volume_24h=Decimal("1000.0"),
                price_change_24h=Decimal("500.0"),
                price_change_percent_24h=Decimal("1.12"),
                high_24h=Decimal("45500.00"),
                low_24h=Decimal("44500.00"),
                timestamp=datetime.now(timezone.utc),
            )

            # Track test object
            self.track_test_object("database_symbol", "BTCUSDT", "current_prices")
            self.track_test_object("redis_key", "price:BTCUSDT")
            self.track_test_object("redis_key", "ticker:BTCUSDT")

            # Process through pipeline
            await pipeline._process_single_ticker("BTCUSDT", test_ticker)

            # Test database storage
            stored_price = await test_manager.postgres.fetchval(
                "SELECT price FROM helios_trading.current_prices WHERE symbol = $1",
                "BTCUSDT",
            )

            if not stored_price or abs(stored_price - test_ticker.price) > Decimal(
                "0.01"
            ):
                raise Exception(
                    f"Database storage failed: expected {test_ticker.price}, got {stored_price}"
                )

            # Test Redis caching
            cached_price = await test_manager.redis.get("price:BTCUSDT")
            if not cached_price or abs(
                Decimal(cached_price) - test_ticker.price
            ) > Decimal("0.01"):
                raise Exception(
                    f"Redis caching failed: expected {test_ticker.price}, got {cached_price}"
                )

            # Test current price retrieval
            retrieved_price = await pipeline.get_current_price("BTCUSDT")
            if not retrieved_price or abs(
                retrieved_price - test_ticker.price
            ) > Decimal("0.01"):
                raise Exception(
                    f"Price retrieval failed: expected {test_ticker.price}, got {retrieved_price}"
                )

            self.print_status("âœ“ PostgreSQL data storage working")
            self.print_status("âœ“ Redis caching working")
            self.print_status("âœ“ Price retrieval working")

            # Clean up our test connection manager
            await test_manager.disconnect_all()

            processing_time = time.time() - start_time

            self.record_test_result(
                "Market Data Pipeline",
                True,
                metrics={
                    "processing_time": processing_time,
                    "database_response_time": 0.1,  # Placeholder
                    "redis_response_time": 0.05,  # Placeholder
                },
            )

            return True

        except Exception as e:
            self.record_test_result("Market Data Pipeline", False, str(e))
            return False

    async def test_data_quality_monitoring(self) -> bool:
        """Test 5: Data Quality Monitoring."""
        self.print_header("Test 5: Data Quality Monitoring")

        try:
            start_time = time.time()

            # Create a fresh connection manager for this test
            from src.data.connection_managers import ConnectionManager

            test_manager = ConnectionManager()
            await test_manager.connect_all()

            # Create test pipeline with isolated connection
            pipeline = MarketDataPipeline()
            pipeline.connection_manager = test_manager  # Use our fresh manager

            # Initialize Binance client
            from src.api.binance_client import BinanceClient

            pipeline.binance_client = BinanceClient(pipeline.config)

            # Create test ticker data
            test_ticker = TickerData(
                symbol="QUALITYTEST",
                price=Decimal("50000.00"),
                bid_price=Decimal("49999.00"),
                ask_price=Decimal("50001.00"),
                volume_24h=Decimal("1000.0"),
                price_change_24h=Decimal("500.0"),
                price_change_percent_24h=Decimal("1.0"),
                high_24h=Decimal("50500.00"),
                low_24h=Decimal("49500.00"),
                timestamp=datetime.now(timezone.utc),
            )

            # Track test object
            self.track_test_object("database_symbol", "QUALITYTEST", "current_prices")
            self.track_test_object(
                "database_symbol", "QUALITYTEST", "data_quality_metrics"
            )

            # Process the ticker through quality pipeline
            await pipeline._process_single_ticker("QUALITYTEST", test_ticker)

            # Verify data was stored correctly
            current_price = await pipeline.get_current_price("QUALITYTEST")
            if not current_price or current_price != test_ticker.price:
                raise Exception(
                    f"Price mismatch: expected {test_ticker.price}, got {current_price}"
                )

            # Check data quality metrics were created
            quality_metrics = await test_manager.postgres.fetch(
                "SELECT * FROM helios_trading.data_quality_metrics WHERE symbol = $1",
                "QUALITYTEST",
            )

            if not quality_metrics:
                raise Exception("No quality metrics found")

            # Verify quality score
            quality_score = quality_metrics[0]["quality_score"]
            if quality_score < 0.8:  # Should be high quality test data
                raise Exception(f"Quality score too low: {quality_score}")

            # Clean up our test connection manager
            await test_manager.disconnect_all()

            processing_time = time.time() - start_time

            self.record_test_result(
                "Data Quality Monitoring",
                True,
                metrics={
                    "processing_time": processing_time,
                    "quality_score": float(quality_score),
                    "metrics_created": len(quality_metrics),
                },
            )

            return True

        except Exception as e:
            self.record_test_result("Data Quality Monitoring", False, str(e))
            return False

    async def test_performance_benchmarks(self) -> bool:
        """Test 6: Performance Benchmarks."""
        self.print_header("Test 6: Performance Benchmarks")

        pipeline = None
        try:
            pipeline = MarketDataPipeline()
            await pipeline.initialize()

            # Benchmark data processing speed
            start_time = time.time()

            test_tickers = []
            for i in range(10):  # Process 10 tickers
                symbol = f"BENCH{i:02d}USDT"

                # Track test objects for cleanup
                self.track_test_object("database_symbol", symbol, "current_prices")
                self.track_test_object(
                    "database_symbol", symbol, "data_quality_metrics"
                )
                self.track_test_object("redis_key", f"price:{symbol}")
                self.track_test_object("redis_key", f"ticker:{symbol}")

                ticker = TickerData(
                    symbol=symbol,
                    price=Decimal(f"{1000 + i}.{i:02d}"),
                    bid_price=Decimal(f"{999 + i}.{i:02d}"),
                    ask_price=Decimal(f"{1001 + i}.{i:02d}"),
                    volume_24h=Decimal(f"{10000 + i * 100}.0"),
                    price_change_24h=Decimal(f"{i}.{i:02d}"),
                    price_change_percent_24h=Decimal(f"{i * 0.1:.2f}"),
                    high_24h=Decimal(f"{1010 + i}.{i:02d}"),
                    low_24h=Decimal(f"{990 + i}.{i:02d}"),
                    timestamp=datetime.now(timezone.utc),
                )
                test_tickers.append((symbol, ticker))

            # Process all tickers
            tasks = []
            for symbol, ticker in test_tickers:
                tasks.append(pipeline._process_single_ticker(symbol, ticker))

            await asyncio.gather(*tasks)

            processing_time = time.time() - start_time
            throughput = len(test_tickers) / processing_time

            self.print_status(
                f"Processed {len(test_tickers)} tickers in {processing_time:.2f}s"
            )
            self.print_status(f"Throughput: {throughput:.1f} tickers/second")

            # Note: Cleanup is now handled centrally, no manual cleanup needed here

            # Performance benchmarks
            benchmarks = {
                "processing_time_seconds": processing_time,
                "throughput_tickers_per_second": throughput,
                "tickers_processed": len(test_tickers),
            }

            # Check performance requirements
            if throughput >= 5.0:  # Should process at least 5 tickers/second
                self.record_test_result(
                    "Performance Benchmarks", True, metrics=benchmarks
                )
                return True
            else:
                self.record_test_result(
                    "Performance Benchmarks",
                    False,
                    f"Throughput too low: {throughput:.1f} < 5.0 tickers/second",
                    benchmarks,
                )
                return False

        except Exception as e:
            self.record_test_result("Performance Benchmarks", False, str(e))
            return False
        finally:
            if pipeline:
                await pipeline.stop_pipeline()

    def print_final_results(self):
        """Print final test results summary."""
        duration = datetime.now() - self.results["start_time"]

        print(f"\n{'='*60}")
        print("ðŸ PHASE 1.3 DATA PIPELINE TEST RESULTS")
        print(f"{'='*60}")

        print("ðŸ“Š Test Summary:")
        print(f"   â€¢ Total Tests: {self.results['tests_run']}")
        print(f"   â€¢ Passed: {self.results['tests_passed']} âœ…")
        print(f"   â€¢ Failed: {self.results['tests_failed']} âŒ")
        print(
            f"   â€¢ Success Rate: {(self.results['tests_passed']/self.results['tests_run']*100):.1f}%"
        )
        print(f"   â€¢ Duration: {duration.total_seconds():.1f} seconds")

        if self.results["performance_metrics"]:
            print("\nâš¡ Performance Metrics:")
            for test_name, metrics in self.results["performance_metrics"].items():
                print(f"   â€¢ {test_name}:")
                for metric, value in metrics.items():
                    if isinstance(value, float):
                        print(f"     - {metric}: {value:.2f}")
                    else:
                        print(f"     - {metric}: {value}")

        if self.results["errors"]:
            print(f"\nðŸš¨ Errors ({len(self.results['errors'])}):")
            for error in self.results["errors"]:
                print(f"   â€¢ {error['test']}: {error['error']}")

        # Overall status
        if self.results["tests_failed"] == 0:
            print("\nðŸŽ‰ ALL TESTS PASSED! Phase 1.3 is ready for production.")
            print("   Your data pipeline is fully functional and performant.")
        else:
            print(
                f"\nâš ï¸  {self.results['tests_failed']} test(s) failed. Please review errors above."
            )

        print("\nðŸ’¡ Next Steps:")
        if self.results["tests_failed"] == 0:
            print("   1. Review performance metrics above")
            print("   2. Monitor free tier usage limits")
            print("   3. Start Phase 1.4 - Real-time Data Collection")
        else:
            print("   1. Fix failed tests")
            print("   2. Re-run: python scripts/test_data_pipeline.py")
            print("   3. Check credentials and network connectivity")


async def main():
    """Run the complete data pipeline test suite."""
    print("ðŸš€ Helios Trading Bot - Phase 1.3 Data Pipeline Testing")
    print("=" * 60)

    test_suite = PipelineTestSuite()

    try:
        # Run all tests in sequence - call and await each test individually
        # Test 1: Configuration
        success = await test_suite.test_configuration()
        if not success:
            test_suite.print_final_results()
            return

        # Test 2: Connection Managers
        success = await test_suite.test_connection_managers()
        if not success:
            test_suite.print_final_results()
            return

        # Test 3: Database Schema (only if connections work)
        success = await test_suite.test_database_schema()
        if not success:
            test_suite.print_final_results()
            return

        # Test 4: Market Data Pipeline (only if database works)
        success = await test_suite.test_market_data_pipeline()
        if not success:
            test_suite.print_final_results()
            return

        # Test 5: Data Quality Monitoring (only if pipeline works)
        success = await test_suite.test_data_quality_monitoring()
        if not success:
            test_suite.print_final_results()
            return

        # Test 6: Performance Benchmarks (only if quality monitoring works)
        success = await test_suite.test_performance_benchmarks()

    except KeyboardInterrupt:
        test_suite.print_status("Testing interrupted by user", "warning")
    except Exception as e:
        test_suite.print_status(f"Testing failed with unexpected error: {e}", "error")

    finally:
        # Always cleanup test objects regardless of test results
        await test_suite.cleanup_all_test_objects()
        test_suite.print_final_results()


def run():
    """Entry point for console script."""
    asyncio.run(main())


if __name__ == "__main__":
    run()
